from flask import Flask, request, jsonify
import os
import constants
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain_community.llms import openai
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from flask_cors import CORS

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# Set OpenAI API key
os.environ["OPENAI_API_KEY"] = constants.APIKEY

# Initialize the embeddings model
embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')

# Load and index the data
loader = TextLoader('data.txt')
index = VectorstoreIndexCreator(embedding=embeddings_model).from_loaders([loader])

# Initialize the language model
llm_gpt = ChatOpenAI(
    model="gpt-4",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=constants.APIKEY
)

# Create a retrieval-based QA chain using the indexed data
qa_chain = RetrievalQA.from_chain_type(
    llm=llm_gpt,
    retriever=index.vectorstore.as_retriever(),
    chain_type="stuff"
)

@app.route('/')
def home():
    return "welcome! use /predict"

@app.route('/predict', methods=['POST'])
def predict():
    query = request.data.decode('utf-8')  # Decoding from byte to string
    response_from_data = qa_chain.run(query)
    context = f"The following information is retrieved from your data:\n{response_from_data}\n\nNow, please answer the following question as if you were Conner Morgan based on both this context and your own knowledge, maintaining a brief and friendly, yet professional tone: {query}"
    response_from_llm_with_context = llm_gpt(messages=[{"role": "user", "content": context}])
    return jsonify({"content": response_from_llm_with_context.content})


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5001)





# import os
# import sys
# import constants
# from langchain_openai import OpenAIEmbeddings
# from langchain_community.document_loaders import TextLoader
# from langchain.indexes import VectorstoreIndexCreator
# from langchain_community.llms import openai
# from langchain_openai import ChatOpenAI
# from langchain.chains import RetrievalQA

# # Set OpenAI API key
# os.environ["OPENAI_API_KEY"] = constants.APIKEY

# # Initialize the embeddings model
# embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')

# # Load and index the data
# loader = TextLoader('data.txt')
# index = VectorstoreIndexCreator(embedding=embeddings_model).from_loaders([loader])

# # Initialize the language model
# llm_gpt = ChatOpenAI(
#     model="gpt-4",
#     temperature=0,
#     max_tokens=None,
#     timeout=None,
#     max_retries=2,
#     api_key=constants.APIKEY
# )

# # Create a retrieval-based QA chain using the indexed data
# qa_chain = RetrievalQA.from_chain_type(
#     llm=llm_gpt,
#     retriever=index.vectorstore.as_retriever(),
#     chain_type="stuff"
# )

# # Get the user query
# query = sys.argv[1]
# # print(f"Query: {query}")

# # Step 1: Try retrieving information from data.txt
# response_from_data = qa_chain.run(query)

# # Step 2: Use the response from data.txt as context for the OpenAI model to generate a more integrated response
# context = f"The following information is retrieved from your data:\n{response_from_data}\n\nNow, please answer the following question as if you were Conner Morgan based on both this context and your own knowledge, maintaining a brief and friendly, yet professional tone: {query}"

# response_from_llm_with_context = llm_gpt(
#     messages=[{"role": "user", "content": context}]
# )

# # Step 3: Output the final response generated by the LLM that incorporates the `data.txt` content
# print(f"{response_from_llm_with_context.content}")